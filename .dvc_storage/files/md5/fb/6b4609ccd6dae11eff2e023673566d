title,authors,journal,year,doi,abstract,keywords,cited_by,methodology,dataset_used,arxiv_id,arxiv_categories,published_date,updated_date,pdf_url,arxiv_url
Generative Refocusing: Flexible Defocus Control from a Single Image,"Chun-Wei Tuan Mu, Jia-Bin Huang, Yu-Lun Liu",arXiv:2512.16923v1,2025,http://arxiv.org/abs/2512.16923v1,"Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.",cs.CV,0,cs.CV,N/A,2512.16923v1,cs.CV,2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16923v1,http://arxiv.org/abs/2512.16923v1
"The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text","Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen",arXiv:2512.16924v1,2025,http://arxiv.org/abs/2512.16924v1,"We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.",cs.CV,0,cs.CV,N/A,2512.16924v1,cs.CV,2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16924v1,http://arxiv.org/abs/2512.16924v1
Next-Embedding Prediction Makes Strong Vision Learners,"Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu",arXiv:2512.16922v1,2025,http://arxiv.org/abs/2512.16922v1,"Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.",cs.CV,0,cs.CV,N/A,2512.16922v1,cs.CV,2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16922v1,http://arxiv.org/abs/2512.16922v1
EasyV2V: A High-quality Instruction-based Video Editing Framework,"Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei",arXiv:2512.16920v1,2025,http://arxiv.org/abs/2512.16920v1,"While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/","cs.CV, cs.AI",0,"cs.CV, cs.AI",N/A,2512.16920v1,"cs.CV, cs.AI",2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16920v1,http://arxiv.org/abs/2512.16920v1
DVGT: Driving Visual Geometry Transformer,"Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu",arXiv:2512.16919v1,2025,http://arxiv.org/abs/2512.16919v1,"Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.","cs.CV, cs.AI, cs.RO",0,"cs.CV, cs.AI, cs.RO",N/A,2512.16919v1,"cs.CV, cs.AI, cs.RO",2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16919v1,http://arxiv.org/abs/2512.16919v1
Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification,"Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu",arXiv:2512.16921v1,2025,http://arxiv.org/abs/2512.16921v1,"Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.","cs.CV, cs.AI",0,"cs.CV, cs.AI",N/A,2512.16921v1,"cs.CV, cs.AI",2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16921v1,http://arxiv.org/abs/2512.16921v1
AdaTooler-V: Adaptive Tool-Use for Images and Videos,"Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue",arXiv:2512.16918v1,2025,http://arxiv.org/abs/2512.16918v1,"Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.",cs.CV,0,cs.CV,N/A,2512.16918v1,cs.CV,2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16918v1,http://arxiv.org/abs/2512.16918v1
Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning,"Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille",arXiv:2512.16917v1,2025,http://arxiv.org/abs/2512.16917v1,"Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.","cs.AI, cs.CL, cs.LG",0,"cs.AI, cs.CL, cs.LG",N/A,2512.16917v1,"cs.AI, cs.CL, cs.LG",2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16917v1,http://arxiv.org/abs/2512.16917v1
Discovering gravitational waveform distortions from lensing: a deep dive into GW231123,"Juno C. L. Chan, Jose María Ezquiaga, Rico K. L. Lo, Joey Bowman, Lorena Magaña Zertuche, Luka Vujeva",arXiv:2512.16916v1,2025,http://arxiv.org/abs/2512.16916v1,"Gravitational waves (GWs) are unique messengers as they travel through the Universe without alteration except for gravitational lensing. Their long wavelengths make them susceptible to diffraction by cosmic structures, providing an unprecedented opportunity to map dark matter substructures. Identifying lensed events requires the analysis of thousands to millions of simulated events to reach high statistical significances. This is computationally prohibitive with standard GW parameter estimation methods. We build on top of state-of-the-art neural posterior algorithms to accelerate the lensed inference from CPU days to minutes with DINGO-lensing. We showcase its capabilities by reanalyzing GW231123, the most promising lensed candidate so far, and find that its statistical significance cannot exceed 4$σ$. We observe that 8% of GW231123-like nonlensed simulations favor lensing, which could be explained by the self-similarity of short-duration signals. Still, 58% of GW231123-like lensed simulations have larger support for lensing, showing that higher detection statistics are possible. Although GW231123 exposes the challenges of claiming the first GW lensing detection, our deep-learning methods have demonstrated to be powerful enough to enable the upcoming discovery of lensed GWs.","astro-ph.CO, hep-ph",0,"astro-ph.CO, hep-ph",N/A,2512.16916v1,"astro-ph.CO, hep-ph",2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16916v1,http://arxiv.org/abs/2512.16916v1
StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors,"Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang, Donghao Zhou, Zhen Yang, Luozhou Wang, Xin Tao, Ying-Cong Chen",arXiv:2512.16915v1,2025,http://arxiv.org/abs/2512.16915v1,"The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.",cs.CV,0,cs.CV,N/A,2512.16915v1,cs.CV,2025-12-18,2025-12-18,https://arxiv.org/pdf/2512.16915v1,http://arxiv.org/abs/2512.16915v1
