#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ ML.
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
"""

from pathlib import Path

import yaml

# –ë–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
ALGORITHM_CONFIGS = {
    "RandomForestClassifier": {
        "train": {
            "algorithm": "RandomForestClassifier",
            "random_forest": {
                "n_estimators": 100,
                "max_depth": 10,
                "min_samples_split": 5,
                "min_samples_leaf": 2,
                "max_features": "sqrt",
                "bootstrap": True,
                "oob_score": True,
            },
        },
        "mlflow": {
            "experiment_name": "random_forest_experiment",
            "tags": {"algorithm": "RandomForest", "complexity": "medium"},
        },
    },
    "SVM": {
        "train": {
            "algorithm": "SVM",
            "svm": {
                "kernel": "rbf",
                "C": 1.0,
                "gamma": "scale",
                "probability": True,
            },
        },
        "mlflow": {
            "experiment_name": "svm_experiment",
            "tags": {"algorithm": "SVM", "complexity": "high"},
        },
    },
    "LogisticRegression": {
        "train": {
            "algorithm": "LogisticRegression",
            "logistic_regression": {
                "penalty": "l2",
                "C": 1.0,
                "max_iter": 1000,
                "solver": "liblinear",
            },
        },
        "mlflow": {
            "experiment_name": "logistic_regression_experiment",
            "tags": {"algorithm": "LogisticRegression", "complexity": "low"},
        },
    },
}

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
DATA_SIZE_CONFIGS = {
    "small": {
        "data": {"max_results": 50},
        "feature_engineering": {"tfidf_max_features": 1000},
    },
    "medium": {
        "data": {"max_results": 100},
        "feature_engineering": {"tfidf_max_features": 5000},
    },
    "large": {
        "data": {"max_results": 500},
        "feature_engineering": {"tfidf_max_features": 10000},
    },
}


def load_base_config(config_path="params.yaml"):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    try:
        with open(config_path, encoding="utf-8") as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print(f"‚ö†Ô∏è –§–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
        return get_minimal_config()


def get_minimal_config():
    """–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –±–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"""
    return {
        "data": {
            "query": "cat:eess.IV OR cat:cs.CV OR cat:q-bio.QM",
            "max_results": 100,
            "source": "arxiv",
        },
        "feature_engineering": {
            "tfidf_max_features": 5000,
            "text_columns": ["title", "abstract"],
            "categorical_columns": ["journal_category"],
            "numerical_columns": ["year", "author_count"],
        },
        "train": {"test_size": 0.2, "random_state": 42},
        "mlflow": {"tracking_uri": "file:./mlruns"},
        "evaluate": {
            "target_column": "arxiv_categories",
            "metrics": ["accuracy", "precision", "recall", "f1_score"],
        },
    }


def deep_merge_dicts(dict1, dict2):
    """–ì–ª—É–±–æ–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä–µ–π"""
    result = dict1.copy()

    for key, value in dict2.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge_dicts(result[key], value)
        else:
            result[key] = value

    return result


def compose_config(algorithm, data_size="medium", base_config_path="params.yaml"):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    base_config = load_base_config(base_config_path)

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏
    algorithm_config = ALGORITHM_CONFIGS.get(algorithm, {})
    data_size_config = DATA_SIZE_CONFIGS.get(data_size, {})

    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç –±–∞–∑–æ–≤—ã–µ)
    result = deep_merge_dicts(base_config, algorithm_config)
    result = deep_merge_dicts(result, data_size_config)

    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    result["_metadata"] = {
        "algorithm": algorithm,
        "data_size": data_size,
        "generated_by": "simple_composer.py",
    }

    return result


def validate_config(config):
    """–ü—Ä–æ—Å—Ç–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    required_sections = ["data", "train", "mlflow"]

    for section in required_sections:
        if section not in config:
            return False, f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å–µ–∫—Ü–∏—è: {section}"

    if "algorithm" not in config["train"]:
        return False, "–ù–µ —É–∫–∞–∑–∞–Ω –∞–ª–≥–æ—Ä–∏—Ç–º –≤ —Å–µ–∫—Ü–∏–∏ train"

    return True, "–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞"


def save_config(config, output_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)

        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        return False


def generate_all_configs(base_config_path="params.yaml"):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤"""
    print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤...")

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    configs_dir = Path("config/generated")
    configs_dir.mkdir(exist_ok=True, parents=True)

    success_count = 0
    total_count = 0

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    for algorithm in ALGORITHM_CONFIGS.keys():
        for data_size in ["small", "medium"]:
            total_count += 1

            print(f"üìù –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {algorithm} ({data_size})")

            # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            config = compose_config(algorithm, data_size, base_config_path)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            is_valid, message = validate_config(config)
            if not is_valid:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {algorithm}: {message}")
                continue

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            filename = f"{algorithm.lower()}_{data_size}_config.yaml"
            config_path = configs_dir / filename

            if save_config(config, config_path):
                print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {config_path}")
                success_count += 1
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {config_path}")

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {success_count}/{total_count} –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —Å–æ–∑–¥–∞–Ω–æ —É—Å–ø–µ—à–Ω–æ")

    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
    create_config_summary(configs_dir)


def create_config_summary(configs_dir):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
    summary = {
        "generated_configs": {
            "description": "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤",
            "algorithms": list(ALGORITHM_CONFIGS.keys()),
            "data_sizes": list(DATA_SIZE_CONFIGS.keys()),
            "total_configs": len(list(configs_dir.glob("*.yaml"))),
            "usage": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å automated_pipeline.py --config <path>",
        }
    }

    summary_path = configs_dir / "README.yaml"
    save_config(summary, summary_path)
    print(f"üìã –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {summary_path}")


def list_generated_configs():
    """–°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π"""
    configs_dir = Path("config/generated")

    if not configs_dir.exists():
        print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return

    config_files = list(configs_dir.glob("*.yaml"))

    if not config_files:
        print("‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return

    print("üìÅ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
    for config_file in sorted(config_files):
        if config_file.name != "README.yaml":
            print(f"   ‚Ä¢ {config_file.name}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ü—Ä–æ—Å—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–æ–º–ø–æ–∑–∏—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π")
    print("=" * 50)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
    generate_all_configs()

    print("\n" + "=" * 50)

    # –°–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
    list_generated_configs()

    print("\n‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(
        "üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python scripts/automated_pipeline.py --config config/generated/<filename>"
    )


if __name__ == "__main__":
    main()
