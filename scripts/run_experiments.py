#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è —Å–µ—Ä–∏–∏ ML —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç 15+ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –ª–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ MLflow.
"""

import copy
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import yaml
from train_model import train_model

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("experiments.log"), logging.StreamHandler()],
)

logger = logging.getLogger(__name__)


def load_base_params(params_file: str = "params.yaml") -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ñ–∞–π–ª–∞."""
    with open(params_file, "r") as f:
        return yaml.safe_load(f)


def create_experiment_configs() -> List[Dict[str, Any]]:
    """–°–æ–∑–¥–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
    base_params = load_base_params()

    experiments = []

    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å Random Forest
    rf_configs = [
        # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        {
            "name": "RF_baseline",
            "algorithm": "RandomForestClassifier",
            "params": {"n_estimators": 100, "max_depth": 10, "min_samples_split": 5},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –ë–æ–ª—å—à–µ –¥–µ—Ä–µ–≤—å–µ–≤
        {
            "name": "RF_more_trees",
            "algorithm": "RandomForestClassifier",
            "params": {"n_estimators": 200, "max_depth": 10, "min_samples_split": 5},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –ì–ª—É–±–∂–µ –¥–µ—Ä–µ–≤—å—è
        {
            "name": "RF_deeper",
            "algorithm": "RandomForestClassifier",
            "params": {"n_estimators": 100, "max_depth": 20, "min_samples_split": 5},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        {
            "name": "RF_conservative",
            "algorithm": "RandomForestClassifier",
            "params": {"n_estimators": 150, "max_depth": 7, "min_samples_split": 10},
            "feature_params": {"tfidf_max_features": 3000, "ngram_range": [1, 2]},
        },
        # –ë–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        {
            "name": "RF_more_features",
            "algorithm": "RandomForestClassifier",
            "params": {"n_estimators": 100, "max_depth": 10, "min_samples_split": 5},
            "feature_params": {"tfidf_max_features": 10000, "ngram_range": [1, 3]},
        },
    ]

    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å SVM
    svm_configs = [
        # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        {
            "name": "SVM_baseline",
            "algorithm": "SVM",
            "params": {"kernel": "rbf", "C": 1.0, "gamma": "scale"},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –õ–∏–Ω–µ–π–Ω–æ–µ —è–¥—Ä–æ
        {
            "name": "SVM_linear",
            "algorithm": "SVM",
            "params": {"kernel": "linear", "C": 1.0, "gamma": "scale"},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –í—ã—Å–æ–∫–∏–π C
        {
            "name": "SVM_high_C",
            "algorithm": "SVM",
            "params": {"kernel": "rbf", "C": 10.0, "gamma": "scale"},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –ù–∏–∑–∫–∏–π C
        {
            "name": "SVM_low_C",
            "algorithm": "SVM",
            "params": {"kernel": "rbf", "C": 0.1, "gamma": "scale"},
            "feature_params": {"tfidf_max_features": 3000, "ngram_range": [1, 2]},
        },
        # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —è–¥—Ä–æ
        {
            "name": "SVM_poly",
            "algorithm": "SVM",
            "params": {"kernel": "poly", "C": 1.0, "gamma": "scale", "degree": 3},
            "feature_params": {"tfidf_max_features": 3000, "ngram_range": [1, 2]},
        },
    ]

    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å Logistic Regression
    lr_configs = [
        # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        {
            "name": "LR_baseline",
            "algorithm": "LogisticRegression",
            "params": {"penalty": "l2", "C": 1.0, "solver": "liblinear"},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # L1 —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        {
            "name": "LR_l1_penalty",
            "algorithm": "LogisticRegression",
            "params": {"penalty": "l1", "C": 1.0, "solver": "liblinear"},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
        # –í—ã—Å–æ–∫–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        {
            "name": "LR_high_reg",
            "algorithm": "LogisticRegression",
            "params": {"penalty": "l2", "C": 0.1, "solver": "liblinear"},
            "feature_params": {"tfidf_max_features": 3000, "ngram_range": [1, 2]},
        },
        # –ù–∏–∑–∫–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        {
            "name": "LR_low_reg",
            "algorithm": "LogisticRegression",
            "params": {"penalty": "l2", "C": 10.0, "solver": "liblinear"},
            "feature_params": {"tfidf_max_features": 7000, "ngram_range": [1, 2]},
        },
        # –î—Ä—É–≥–æ–π —Å–æ–ª–≤–µ—Ä
        {
            "name": "LR_lbfgs",
            "algorithm": "LogisticRegression",
            "params": {"penalty": "l2", "C": 1.0, "solver": "lbfgs"},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 2]},
        },
    ]

    # –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_experiments = [
        # –¢–æ–ª—å–∫–æ —É–Ω–∏–≥—Ä–∞–º–º—ã
        {
            "name": "RF_unigrams_only",
            "algorithm": "RandomForestClassifier",
            "params": {"n_estimators": 100, "max_depth": 10, "min_samples_split": 5},
            "feature_params": {"tfidf_max_features": 5000, "ngram_range": [1, 1]},
        },
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ n-–≥—Ä–∞–º–º—ã
        {
            "name": "LR_extended_ngrams",
            "algorithm": "LogisticRegression",
            "params": {"penalty": "l2", "C": 1.0, "solver": "liblinear"},
            "feature_params": {"tfidf_max_features": 8000, "ngram_range": [1, 4]},
        },
    ]

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    all_configs = rf_configs + svm_configs + lr_configs + feature_experiments

    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    for i, config in enumerate(all_configs):
        params = copy.deepcopy(base_params)

        # –û–±–Ω–æ–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        params["train"]["algorithm"] = config["algorithm"]

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        if config["algorithm"] == "RandomForestClassifier":
            params["train"]["random_forest"].update(config["params"])
        elif config["algorithm"] == "SVM":
            params["train"]["svm"].update(config["params"])
        elif config["algorithm"] == "LogisticRegression":
            params["train"]["logistic_regression"].update(config["params"])

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        params["feature_engineering"].update(config["feature_params"])

        # –û–±–Ω–æ–≤–ª—è–µ–º MLflow –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        params["mlflow"]["run_name"] = f"exp_{i + 1:02d}_{config['name']}"
        params["mlflow"]["tags"]["experiment_type"] = config["name"]
        params["mlflow"]["tags"]["algorithm"] = config["algorithm"]

        experiments.append(
            {
                "name": config["name"],
                "params": params,
                "description": f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {i + 1}: {config['name']} —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {config['params']}",
            }
        )

    return experiments


def run_single_experiment(
    exp_config: Dict[str, Any], data_file: str, base_output_dir: str
) -> Dict[str, Any]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–∏–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
    exp_name = exp_config["name"]
    params = exp_config["params"]

    logger.info(f"–ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {exp_name}")
    logger.info(f"–û–ø–∏—Å–∞–Ω–∏–µ: {exp_config['description']}")

    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    temp_params_file = f"temp_params_{exp_name}.yaml"
    with open(temp_params_file, "w") as f:
        yaml.dump(params, f, default_flow_style=False)

    # –°–æ–∑–¥–∞–µ–º –ø—É—Ç–∏ –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
    output_dir = Path(base_output_dir) / exp_name
    output_dir.mkdir(parents=True, exist_ok=True)

    model_output = output_dir / "model.pkl"
    metrics_output = output_dir / "metrics.json"

    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        start_time = time.time()
        model, test_metrics = train_model(
            data_file=data_file,
            params_file=temp_params_file,
            model_output=str(model_output),
            metrics_output=str(metrics_output),
        )
        end_time = time.time()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        with open(metrics_output, "r") as f:
            metrics = json.load(f)

        result = {
            "experiment_name": exp_name,
            "algorithm": params["train"]["algorithm"],
            "status": "success",
            "training_time": end_time - start_time,
            "test_accuracy": metrics["test_metrics"]["accuracy"],
            "test_f1_score": metrics["test_metrics"]["f1_score"],
            "cv_mean_score": metrics["cross_validation"]["mean_score"],
            "cv_std_score": metrics["cross_validation"]["std_score"],
            "params": params["train"],
            "feature_params": params["feature_engineering"],
            "model_path": str(model_output),
            "metrics_path": str(metrics_output),
        }

        logger.info(f"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {exp_name} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        logger.info(f"   –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {result['test_accuracy']:.4f}")
        logger.info(f"   F1-score: {result['test_f1_score']:.4f}")
        logger.info(f"   –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {result['training_time']:.2f}s")

    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–µ {exp_name}: {str(e)}")
        result = {
            "experiment_name": exp_name,
            "algorithm": params["train"]["algorithm"],
            "status": "failed",
            "error": str(e),
            "params": params["train"],
            "feature_params": params["feature_engineering"],
        }

    finally:
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        Path(temp_params_file).unlink(missing_ok=True)

    return result


def analyze_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
    successful_results = [r for r in results if r["status"] == "success"]

    if not successful_results:
        return {"status": "no_successful_experiments"}

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ test_accuracy
    sorted_by_accuracy = sorted(
        successful_results, key=lambda x: x["test_accuracy"], reverse=True
    )
    sorted_by_f1 = sorted(
        successful_results, key=lambda x: x["test_f1_score"], reverse=True
    )

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º
    algorithm_stats = {}
    for result in successful_results:
        alg = result["algorithm"]
        if alg not in algorithm_stats:
            algorithm_stats[alg] = {"count": 0, "accuracies": [], "f1_scores": []}
        algorithm_stats[alg]["count"] += 1
        algorithm_stats[alg]["accuracies"].append(result["test_accuracy"])
        algorithm_stats[alg]["f1_scores"].append(result["test_f1_score"])

    # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º
    for alg, stats in algorithm_stats.items():
        stats["mean_accuracy"] = sum(stats["accuracies"]) / len(stats["accuracies"])
        stats["mean_f1_score"] = sum(stats["f1_scores"]) / len(stats["f1_scores"])
        stats["best_accuracy"] = max(stats["accuracies"])
        stats["best_f1_score"] = max(stats["f1_scores"])

    analysis = {
        "total_experiments": len(results),
        "successful_experiments": len(successful_results),
        "failed_experiments": len(results) - len(successful_results),
        "best_accuracy_experiment": sorted_by_accuracy[0],
        "best_f1_experiment": sorted_by_f1[0],
        "algorithm_statistics": algorithm_stats,
        "top_5_by_accuracy": sorted_by_accuracy[:5],
        "top_5_by_f1": sorted_by_f1[:5],
    }

    return analysis


def save_experiment_summary(
    results: List[Dict[str, Any]], analysis: Dict[str, Any], output_file: str
):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
    summary = {
        "experiment_run_date": datetime.now().isoformat(),
        "summary": analysis,
        "detailed_results": results,
    }

    with open(output_file, "w") as f:
        json.dump(summary, f, indent=2, default=str)

    logger.info(f"–°–≤–æ–¥–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_file}")


def print_summary(analysis: Dict[str, Any]):
    """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å."""
    print("\n" + "=" * 80)
    print("–°–í–û–î–ö–ê –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–û–í")
    print("=" * 80)

    print(f"–í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {analysis['total_experiments']}")
    print(f"–£—Å–ø–µ—à–Ω—ã—Ö: {analysis['successful_experiments']}")
    print(f"–ù–µ—É–¥–∞—á–Ω—ã—Ö: {analysis['failed_experiments']}")

    print("\nüèÜ –õ–£–ß–®–ò–ô –ü–û –¢–û–ß–ù–û–°–¢–ò:")
    best_acc = analysis["best_accuracy_experiment"]
    print(f"   –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {best_acc['experiment_name']}")
    print(f"   –ê–ª–≥–æ—Ä–∏—Ç–º: {best_acc['algorithm']}")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {best_acc['test_accuracy']:.4f}")
    print(f"   F1-score: {best_acc['test_f1_score']:.4f}")

    print("\nüéØ –õ–£–ß–®–ò–ô –ü–û F1-SCORE:")
    best_f1 = analysis["best_f1_experiment"]
    print(f"   –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {best_f1['experiment_name']}")
    print(f"   –ê–ª–≥–æ—Ä–∏—Ç–º: {best_f1['algorithm']}")
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {best_f1['test_accuracy']:.4f}")
    print(f"   F1-score: {best_f1['test_f1_score']:.4f}")

    print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ê–õ–ì–û–†–ò–¢–ú–ê–ú:")
    for alg, stats in analysis["algorithm_statistics"].items():
        print(f"   {alg}:")
        print(f"      –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {stats['count']}")
        print(f"      –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {stats['mean_accuracy']:.4f}")
        print(f"      –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {stats['best_accuracy']:.4f}")
        print(f"      –°—Ä–µ–¥–Ω–∏–π F1-score: {stats['mean_f1_score']:.4f}")
        print(f"      –õ—É—á—à–∏–π F1-score: {stats['best_f1_score']:.4f}")

    print("\n" + "=" * 80)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤."""
    logger.info("–ù–∞—á–∞–ª–æ —Å–µ—Ä–∏–∏ ML —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    data_file = "data/processed/publications_processed.csv"
    output_dir = "experiments"
    summary_file = "experiments_summary.json"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    if not Path(data_file).exists():
        logger.error(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω: {data_file}")
        return

    # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
    logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤...")
    experiment_configs = create_experiment_configs()
    logger.info(f"–°–æ–∑–¥–∞–Ω–æ {len(experiment_configs)} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")

    # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    Path(output_dir).mkdir(exist_ok=True)

    # –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã
    results = []
    total_start_time = time.time()

    for i, config in enumerate(experiment_configs, 1):
        logger.info(f"\n{'=' * 60}")
        logger.info(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {i}/{len(experiment_configs)}")
        logger.info(f"{'=' * 60}")

        result = run_single_experiment(config, data_file, output_dir)
        results.append(result)

        # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –æ—Ç—á–µ—Ç –∫–∞–∂–¥—ã–µ 5 —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
        if i % 5 == 0:
            successful = sum(1 for r in results if r["status"] == "success")
            logger.info(f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∏—Ç–æ–≥: {successful}/{i} —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ")

    total_time = time.time() - total_start_time
    logger.info(f"\n–í—Å–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    logger.info("–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    analysis = analyze_results(results)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_experiment_summary(results, analysis, summary_file)

    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print_summary(analysis)

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ MLflow
    print("\nüìà –î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ MLflow –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
    print("   mlflow ui --backend-store-uri file:./mlruns --host 127.0.0.1 --port 3000")
    print("   –ó–∞—Ç–µ–º –æ—Ç–∫—Ä–æ–π—Ç–µ: http://127.0.0.1:3000")

    logger.info("–°–µ—Ä–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    main()
